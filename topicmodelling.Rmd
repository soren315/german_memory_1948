---
title: "topicmodelling"
author: "Abu Bakr"
date: "2024-01-02"
output: html_document
---

```{r}
needs(topicmodels, broom, dplyr, tidytext, tidyr, tidyverse, SnowballC, ldatuning)
```

```{r in tidytext-Format bringen}
library(tidytext)
library(tidyverse)
library(SnowballC)

tbl_tidy <- tbl_cleaned |> 
  mutate(text = text |> 
           str_replace_all("[,.]", " ")) |> 
  unnest_tokens(output = token, input = text) |> 
   filter(!str_detect(token, "[:digit:]")) |> 
  anti_join(get_stopwords(language = "de"), by = c("token" = "word")) |> 
  anti_join(get_stopwords(language = "de", source = "stopwords-iso"), by = c("token" = "word")) |> 
  anti_join(get_stopwords(language = "de", source = "marimo"), by = c("token" = "word")) |> 
  mutate(token = wordStem(token, language = "de"))

tbl_tidy <- tbl_tidy |> 
  filter(token != "quot") |> 
  filter(token != "genios:styl") |> 
  filter(token != "dpa")
#hier entferne ich jetzt aber alle Zahlen, auch etwa wichtige Zahlen wie 1948, 1967, 2001, etc. Aber wenn ich Hypothese der Nakba untersuche ist das, glaube ich, nicht wichtig
```




```{r überprüfen und manuell nachbessern}
tbl_tidy |> 
  count(token, sort = TRUE)

tbl_tidy |> 
  group_by(token) |> 
  filter(token == "israel")
```

```{r DTM erstellen}
tbl_dtm <- tbl_tidy |> 
  filter(str_length(token) > 1) |> 
  count(year, token) |> 
  group_by(token) |> 
  filter(n() < 50) |> 
  cast_dtm(document = year, term = token, value = n)

tbl_dtm <- tbl_tidy |> 
  filter(str_length(token) > 1) |> 
  count(year, token) |> 
  group_by(token) |> 
  cast_dtm(document = year, term = token, value = n)
```

```{r}
tbl_dtm |> as.matrix() %>% .[1:5, 1:5]
```
```{r include=FALSE}
lda_k10_tidy <- read_rds("/Users/abuzuzu/Desktop/R_BA/dereko_final/data/lda_k10_tidy.rds")
```


```{r eval=FALSE}
needs(topicmodels, broom)

lda_k10 <- LDA(tbl_dtm, k = 10, control = list(seed = 123))

lda_k10_tidy <- tidy(lda_k10)
#saveRDS(lda_k10_tidy, file = "/Users/abuzuzu/Desktop/R_BA/dereko_final/data/lda_k10_tidy.rds")
```

```{r}
lda_k10_tidy |> glimpse()
```


```{r}
top_terms_k10 <- lda_k10_tidy |>
  group_by(topic) |>
  slice_max(beta, n = 5, with_ties = FALSE) |>
  ungroup() |>
  arrange(topic, -beta)

top_terms_k10 |>
  mutate(topic = factor(topic),
         term = reorder_within(term, beta, topic)) |>
  ggplot(aes(term, beta, fill = topic)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  scale_x_reordered() +
  facet_wrap(~topic, scales = "free", ncol = 2) +
  coord_flip()

ggsave("tm_k10_israel.png", width = 7, height = 5)
#Hier sieht man gerade, dass zb noch immer Sachen wie "aviv" da stehen und es eigentlich cool wäre, "tel_aviv" zu haben
```
```{r}
needs(ldatuning)
```

```{r eval=FALSE}
determine_k <- FindTopicsNumber(
  tbl_dtm,
  topics = seq(from = 2, to = 30, by = 5),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 16L,
  verbose = TRUE
)

#determine_k |> write_rds("lda_tuning.rds")
```

```{r include=FALSE}
determine_k <- read_rds("/Users/abuzuzu/Desktop/R_BA/dereko_final/data/lda_tuning_dereko.rds")
```

```{r}
FindTopicsNumber_plot(determine_k)

ggsave(file = "determine_k_stm.png", width = 8, height = 5)
```
15 topics bei preliminary Versuch am 15.12. Vielleicht sollte ich das Felix mal schicken und schauen, was er sacht

```{r eval=FALSE}
library(lda)
library(topicmodels)
lda_k15 <- LDA(tbl_dtm, k = 15, control = list(seed = 77))

lda_k15_tidied <- tidy(lda_k15)

write_rds(lda_k15, "lda_k15.rds")
```

```{r include=FALSE}
needs(topicmodels)
lda_k15 <- read_rds("/Users/abuzuzu/Desktop/R_BA/dereko_final/data/lda_k15.rds")
lda_k15_tidied <- tidy(lda_k15)
```

## Sense-making

```{r}
topic_list <- lda_k15_tidied |> 
  group_by(topic) |> 
  group_split() |> 
  map_dfc(~.x |> 
            slice_max(beta, n = 20, with_ties = FALSE) |>
            arrange(-beta) |> 
            select(term)) |> 
  set_names(str_c("topic", 1:15, sep = "_"))

#write_csv(topic_list, file = "topic_list.csv")
```
_Das könnte man zB schon ausdrucken und in den Text packen. Dann eben dazuschreiben, dass das noch bereinigt wird, das tun und anschließend erneut abdrucken_
```{r}
topic_list_filtered <- lda_k15_tidied |> 
  group_by(topic) |> 
    filter(!term %in% c("arab", "jud", "judisch", "israel", "palastinens", "staat", "arab", "palastina", )) |>
  group_split() |> 
  map_dfc(~.x |> 
            slice_max(beta, n = 20, with_ties = FALSE) |>
            arrange(-beta) |> 
            select(term)) |> 
  set_names(str_c("topic", 1:15, sep = "_"))

#write_csv(topic_list_filtered, file = "topic_list_filtered.csv")


```

## Document-topic probabilities

Another thing to assess is document-topic probabilities `gamma`: which document belongs to which topic. By doing so, you can choose the documents that have the highest probability of belonging to a topic and then read these specifically. This might give you a better understanding of what the different topics might imply. 

```{r}
lda_k15_document <- tidy(lda_k15, matrix = "gamma")
```
_Hier komisch: Irgendwie gibt es bei Gamma gar kein Jahr 2023, habe ich das unterwegs verloren?_ -> nein, leider gibt es aus 2023 gar keine Artikel

This shows you the proportion of words in the document which were drawn from the specific topics. In 1990, for instance, many words were drawn from the first topic.

```{r}
lda_k15_document |> 
  group_by(document) |> 
  slice_max(gamma, n = 1) |> 
  mutate(gamma = round(gamma, 3))
```
_Besonders auffallend ist das in den Jahren der Intifada (2000-2002) nur Thema 12 bespielt wurde. Danach 2004-2006 vor allem Thema 4. Interessant sind besonders die letzten paar Jahre, dort gibt es aber ein sehr unterschiedliches Bild_
An interesting pattern is that the topics show some time-dependency. This intuitively makes sense, as they might represent some sort of deeper underlying issue.

### `LDAvis`

`LDAvis` is a handy tool we can use to inspect our model visually. Preprocessing the data is a bit tricky though, therefore we define a quick function first.

```{r}
needs(LDAvis)

prep_lda_output <- function(dtm, lda_output){
  doc_length <- dtm |> 
    as.matrix() |> 
    as_tibble() |> 
    rowwise() |> 
    summarize(doc_sum = c_across() |> sum()) |> 
    pull(doc_sum)
  phi <- posterior(lda_output)$terms |> as.matrix()
  theta <- posterior(lda_output)$topics |> as.matrix()
  vocab <- colnames(dtm)
  term_sums <- dtm |> 
    as.matrix() |> 
    as_tibble() |> 
    summarize(across(everything(), ~sum(.x))) |> 
    as.matrix()
  svd_tsne <- function(x) tsne::tsne(svd(x)$u)
  LDAvis::createJSON(phi = phi, 
                     theta = theta,
                     vocab = vocab,
                     doc.length = doc_length,
                     term.frequency = term_sums[1,],
                     mds.method = svd_tsne
  )
}

json_lda <- prep_lda_output(tbl_dtm, lda_k15)
```

```{r eval=FALSE}
serVis(json_lda, out.dir = 'vis', open.browser = TRUE)

servr::daemon_stop(1)
```